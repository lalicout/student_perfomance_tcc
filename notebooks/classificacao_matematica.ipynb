{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d4cd618",
   "metadata": {},
   "source": [
    "# Classificação - Matemática\n",
    "\n",
    "Este notebook realiza a modelagem preditiva da aprovação estudantil na disciplina de **Matemática**, com base em atributos contextuais e de desempenho. A análise inclui:\n",
    "- Preparação dos dados\n",
    "- Avaliação de classificadores binários\n",
    "- Comparação com e sem balanceamento\n",
    "- Otimização de hiperparâmetros\n",
    "\n",
    "**Base:** Student Performance Dataset (UCI)  \n",
    "**Target:** variável `aprovacao` (0 = Reprovado, 1 = Aprovado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfcb263",
   "metadata": {},
   "source": [
    "###### ajustar o path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDE\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "path = pathlib.Path().resolve()\n",
    "while path.name != 'student_perfomance_tcc' and path != path.parent:\n",
    "    path = path.parent\n",
    "\n",
    "# Adicionar a raiz ao sys.path para importar o módulo\n",
    "if str(path) not in sys.path:\n",
    "    sys.path.append(str(path))\n",
    "\n",
    "from ajustar_path import adicionar_modulos_ao_path\n",
    "\n",
    "# Adiciona a pasta 'modulos' ao path\n",
    "adicionar_modulos_ao_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546afcae",
   "metadata": {},
   "source": [
    "# 1. Importação e setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03678546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importação dos Módulos e Funções Desenvolvidos\n",
    "\n",
    "from pre_modelagem import *\n",
    "from modelagem import *\n",
    "# Importação de bibliotecas padrão\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "materia = 'matematica'\n",
    "df_raw = importar_base(materia)\n",
    "df_modelo = preparar_dados(df_raw, scaling=True,columns_to_drop=['nota1','nota2','nota_final'])\n",
    "\n",
    "X = df_modelo.drop(columns='aprovacao').values\n",
    "y = df_modelo['aprovacao'].values\n",
    "\n",
    "\n",
    "# Configurações do Pandas para exibir todas as linhas e colunas no DataFrame \n",
    "# para inspeção detalhada dos dados no Jupyter Notebook\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignora todos os warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba222b4d",
   "metadata": {},
   "source": [
    "# Avaliação dos modelos\n",
    "## Subseção 2.1 - Modelos considerados\n",
    "- Regressão Logística\n",
    "- Árvore de Decisão\n",
    "- Random Forest\n",
    "- SVM (com probabilidade)\n",
    "- Gradient Boosting (substituindo AdaBoost)\n",
    "\n",
    "Avaliamos os modelos com **validação cruzada** e também com **balanceamento SMOTE-Tomek** nos dados de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ae34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Classificadores base ===\n",
    "classificadores = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"SVM\": SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# === Dicionário de hiperparâmetros para otimização ===\n",
    "param_grid_dicts = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"C\": [0.01, 0.1, 1, 10],\n",
    "        \"solver\": [\"lbfgs\", \"liblinear\"],\n",
    "        \"penalty\": [\"l2\"]\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "        \"max_depth\": [3, 5, 10, None],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"criterion\": [\"gini\", \"entropy\"]\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [5, 10, None],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"]\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.3],\n",
    "        \"max_depth\": [2, 3, 5]\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"kernel\": [\"linear\", \"rbf\"],\n",
    "        \"gamma\": [\"scale\", \"auto\"]\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a0a86",
   "metadata": {},
   "source": [
    "# 3.  Comparação de desempenho\n",
    "## 3.1. Visualização dos rankings por métrica\n",
    "Inclui: Acurácia, F1-Score, AUC-ROC e outras métricas para os dois cenários:\n",
    "- Dados originais\n",
    "- Dados balanceados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a287a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Otimizando GradientBoosting com GridSearchCV...\n"
     ]
    }
   ],
   "source": [
    "# === Avaliação SEM balanceamento ===\n",
    "df_resultados, df_cv, best_params_df = avaliar_classificadores_binarios_otimizados(\n",
    "    classificadores=classificadores,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    materia=\"matematica\",  # ou \"matematica\"\n",
    "    usar_balanceamento=False,\n",
    "    param_spaces=param_grid_dicts,\n",
    "    testsize=0.2\n",
    ")\n",
    "\n",
    "# === Avaliação COM balanceamento ===\n",
    "df_resultados_bal, df_cv_bal, best_params_df_bal = avaliar_classificadores_binarios_otimizados(\n",
    "    classificadores=classificadores,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    materia=\"matematica\",  # ou \"matematica\"\n",
    "    usar_balanceamento=True,\n",
    "    param_spaces= param_grid_dicts,\n",
    "    testsize=0.2\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca015c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_resultados)\n",
    "\n",
    "display(df_cv)\n",
    "\n",
    "display(verificar_overfitting(df_resultados, df_cv))\n",
    "\n",
    "metricas_interesse = ['Recall(0)', 'F1 Score (Reprovado)', 'F1 Score (Macro)', 'AUC ROC']\n",
    "\n",
    "\n",
    "for metric in metricas_interesse:\n",
    "    if metric:\n",
    "        print(f\"\\n--- Comparando Resultados para a Métrica: {metric} ---\")\n",
    "        comparar_resultados_classificacao(\n",
    "                                            df_test=df_resultados, \n",
    "                                            df_cv=df_cv,           \n",
    "                                            metrics=metric,        \n",
    "                                            materia=materia  \n",
    "                                         )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef72a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_resultados_bal)\n",
    "\n",
    "display(df_cv_bal)\n",
    "\n",
    "display(verificar_overfitting(df_resultados_bal, df_cv_bal))\n",
    "\n",
    "\n",
    "metricas_interesse = ['Recall(0)', 'F1 Score (Reprovado)', 'F1 Score (Macro)', 'AUC ROC']\n",
    "\n",
    "\n",
    "for metric in metricas_interesse:\n",
    "    if metric:\n",
    "        print(f\"\\n--- Comparando Resultados para a Métrica: {metric} ---\")\n",
    "        comparar_resultados_classificacao(\n",
    "                                            df_test=df_resultados, \n",
    "                                            df_cv=df_cv,           \n",
    "                                            metrics=[metric],        \n",
    "                                            materia=materia  \n",
    "                                         )     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df43ae3c",
   "metadata": {},
   "source": [
    "# 4 . Modelagem 1: Seleção Baseada em Critérios Estatísticos e Relevância Direta\n",
    "\n",
    "**Objetivos:**\n",
    "- Foco na escolha de atributos que demonstram uma relação estatisticamente significativa com a variável alvo ('aprovacao')\n",
    "- multicolinearidade critério de alta relevância."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80a5e8",
   "metadata": {},
   "source": [
    "## 4.1 **METODOLOGIA DA SELEÇÃO DOS ATRIBUTOS**\n",
    "\n",
    "### Seleção Categórica (Qui-Quadrado): Utilizar o teste Qui-Quadrado χ \n",
    "\n",
    "- avaliar a associação entre cada variável categórica (nominal ou binária, após one-hot encoding) e a variável 'aprovacao'. \n",
    "Selecionar apenas as variáveis com valor-p < 0.05 (ou um limiar similar).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd0c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "variaveis_dummies_chi2 = [\n",
    "    'escola_Mousinho da Silveira',\n",
    "    'motivo_escolha_escola_Reputação da escola',\n",
    "    'interesse_ensino_superior',\n",
    "    'profissao_mae_Professor(a)',\n",
    "    'motivo_escolha_escola_Outro motivo'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11349d37",
   "metadata": {},
   "source": [
    "\n",
    "### Seleção Quantitativa/Ordinal (Correlação/Significância):\n",
    "\n",
    "- Calcular a correlação (ex: Spearman) entre as variáveis quantitativas/ordinais e a variável 'aprovacao'. \n",
    "Selecionar aquelas com correlação significativa (considerando um limiar de valor absoluto e/ou valor-p).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d7403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "variaveis_ordinais_relevantes = [\n",
    "    'tempo_estudo', 'reprovacoes', 'tempo_livre',\n",
    "    'frequencia_saidas', 'alcool_fim_semana', 'saude', 'faltas'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ab794f",
   "metadata": {},
   "source": [
    "\n",
    "### Tratamento de Multicolinearidade (VIF/Correlação):\n",
    "- variáveis com alta correlação (>0.6) e VIF > 5 identificados: (escolaridade_mae/pai, alcool_dias_uteis/fim_semana).\n",
    "- Foram escolhidos para serem removidos: escolaridade_pai e alcool_fim_semana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd49400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variaveis_alta_colineariedade = [\n",
    "            'escolaridade_mae',\n",
    "            #'escolaridade_pai',\n",
    "            'alcool_fim_semana',\n",
    "            #'alcool_dias_uteis',\n",
    "            #'profissao_pai_Outra profissão',\n",
    "            #'profissao_pai_Serviços'\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19399ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "selecao = variaveis_ordinais_relevantes + variaveis_dummies_chi2+variaveis_alta_colineariedade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "variaveis_final = sorted(selecao)\n",
    "X = df_modelo[variaveis_final]\n",
    "y = df_modelo['aprovacao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Avaliação SEM balanceamento ===\n",
    "df_resultados, df_cv, best_params_df = avaliar_classificadores_binarios_otimizados(\n",
    "    classificadores=classificadores,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    materia=\"matematica\",  # ou \"matematica\"\n",
    "    usar_balanceamento=False,\n",
    "    param_spaces=param_grid_dicts,\n",
    "    testsize=0.2\n",
    ")\n",
    "\n",
    "# === Avaliação COM balanceamento ===\n",
    "df_resultados_bal, df_cv_bal, best_params_df_bal = avaliar_classificadores_binarios_otimizados(\n",
    "    classificadores=classificadores,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    materia=\"matematica\",  # ou \"matematica\"\n",
    "    usar_balanceamento=True,\n",
    "    param_spaces= param_grid_dicts,\n",
    "    testsize=0.2\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb990347",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_resultados)\n",
    "\n",
    "display(df_cv)\n",
    "\n",
    "vf = verificar_overfitting(df_resultados, df_cv)\n",
    "\n",
    "display(vf)\n",
    "\n",
    "vf =vf[vf['Diagnóstico']=='OK']\n",
    "\n",
    "\n",
    "\n",
    "metricas_interesse = ['Recall(0)', 'F1 Score (Reprovado)', 'F1 Score (Macro)', 'AUC ROC']\n",
    "\n",
    "\n",
    "for metric in metricas_interesse:\n",
    "    if metric:\n",
    "        print(f\"\\n--- Comparando Resultados para a Métrica: {metric} ---\")\n",
    "        comparar_resultados_classificacao(\n",
    "                                            df_test=df_resultados, \n",
    "                                            df_cv=df_cv,           \n",
    "                                            metrics=metric,        \n",
    "                                            materia=materia  \n",
    "                                         )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_resultados_bal)\n",
    "\n",
    "display(df_cv_bal)\n",
    "\n",
    "display(verificar_overfitting(df_resultados_bal, df_cv_bal))\n",
    "\n",
    "\n",
    "metricas_interesse = ['Recall(0)', 'F1 Score (Reprovado)', 'F1 Score (Macro)', 'AUC ROC']\n",
    "\n",
    "\n",
    "for metric in metricas_interesse:\n",
    "    if metric:\n",
    "        print(f\"\\n--- Comparando Resultados para a Métrica: {metric} ---\")\n",
    "        comparar_resultados_classificacao(\n",
    "                                            df_test=df_resultados, \n",
    "                                            df_cv=df_cv,           \n",
    "                                            metrics=[metric],        \n",
    "                                            materia=materia  \n",
    "                                         )     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b88c64",
   "metadata": {},
   "source": [
    "# 5. Modelagem 2: Seleção Baseada em Impacto Observado na EDA e Relevância Contextual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e4d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "variaveis_matematica = [\n",
    "    'reprovacoes',\n",
    "    'escolaridade_mae',\n",
    "    'escolaridade_pai',\n",
    "    'alcool_dias_uteis',\n",
    "    'tempo_estudo',\n",
    "    'profissao_mae_Outra profissão', 'profissao_mae_Professor(a)',\n",
    "    'profissao_mae_Serviços', 'profissao_mae_Área da saúde',\n",
    "    'profissao_pai_Outra profissão', 'profissao_pai_Professor(a)',\n",
    "    'profissao_pai_Serviços', 'profissao_pai_Área da saúde',\n",
    "    'tempo_transporte',\n",
    "    'frequencia_saidas',\n",
    "    'faltas',\n",
    "    'interesse_ensino_superior',\n",
    "    'genero_Mulher'  # variável secundária\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b73e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_modelo[variaveis_matematica]\n",
    "y = df_modelo['aprovacao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Avaliação SEM balanceamento ===\n",
    "df_resultados, df_cv, best_params_df = avaliar_classificadores_binarios_otimizados(\n",
    "    classificadores=classificadores,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    materia=\"matematica\",  # ou \"matematica\"\n",
    "    usar_balanceamento=False,\n",
    "    param_spaces=param_grid_dicts\n",
    ")\n",
    "\n",
    "# === Avaliação COM balanceamento ===\n",
    "df_resultados_bal, df_cv_bal, best_params_df_bal = avaliar_classificadores_binarios_otimizados(\n",
    "    classificadores=classificadores,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    materia=\"matematica\",  # ou \"matematica\"\n",
    "    usar_balanceamento=True,\n",
    "    param_spaces= param_grid_dicts\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_resultados)\n",
    "\n",
    "display(df_cv)\n",
    "\n",
    "display(verificar_overfitting(df_resultados, df_cv))\n",
    "\n",
    "metricas_interesse = ['Recall(0)', 'F1 Score (Reprovado)', 'F1 Score (Macro)', 'AUC ROC']\n",
    "\n",
    "\n",
    "for metric in metricas_interesse:\n",
    "    if metric:\n",
    "        print(f\"\\n--- Comparando Resultados para a Métrica: {metric} ---\")\n",
    "        comparar_resultados_classificacao(\n",
    "                                            df_test=df_resultados, \n",
    "                                            df_cv=df_cv,           \n",
    "                                            metrics=metric,        \n",
    "                                            materia=materia  \n",
    "                                         )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_resultados_bal)\n",
    "\n",
    "display(df_cv_bal)\n",
    "\n",
    "display(verificar_overfitting(df_resultados_bal, df_cv_bal))\n",
    "\n",
    "\n",
    "metricas_interesse = ['Recall(0)', 'F1 Score (Reprovado)', 'F1 Score (Macro)', 'AUC ROC']\n",
    "\n",
    "\n",
    "for metric in metricas_interesse:\n",
    "    if metric:\n",
    "        print(f\"\\n--- Comparando Resultados para a Métrica: {metric} ---\")\n",
    "        comparar_resultados_classificacao(\n",
    "                                            df_test=df_resultados, \n",
    "                                            df_cv=df_cv,           \n",
    "                                            metrics=[metric],        \n",
    "                                            materia=materia  \n",
    "                                         )     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
